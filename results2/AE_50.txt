Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 227.334768
Pre-training layer 0, epoch 1, cost 103.684284
Pre-training layer 0, epoch 2, cost 102.009807
Pre-training layer 0, epoch 3, cost 101.359752
Pre-training layer 0, epoch 4, cost 101.016824
Pre-training layer 0, epoch 5, cost 100.806486
Pre-training layer 0, epoch 6, cost 100.665107
Pre-training layer 0, epoch 7, cost 100.564005
Pre-training layer 0, epoch 8, cost 100.488386
Pre-training layer 0, epoch 9, cost 100.429869
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.90
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.76543 acc, 0.77333 prec, 0.85926 f1
SVM-RBF 0.81481 acc, 0.81333 prec, 0.89051 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 224.415356
Pre-training layer 0, epoch 1, cost 101.458615
Pre-training layer 0, epoch 2, cost 99.774527
Pre-training layer 0, epoch 3, cost 99.120482
Pre-training layer 0, epoch 4, cost 98.775326
Pre-training layer 0, epoch 5, cost 98.563553
Pre-training layer 0, epoch 6, cost 98.421167
Pre-training layer 0, epoch 7, cost 98.319317
Pre-training layer 0, epoch 8, cost 98.243119
Pre-training layer 0, epoch 9, cost 98.184138
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000.0, 'gamma': 1000.0} with a score of 0.93
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.84774 acc, 0.84753 prec, 0.91084 f1
SVM-RBF 0.87654 acc, 0.86996 prec, 0.92823 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 223.438913
Pre-training layer 0, epoch 1, cost 101.904392
Pre-training layer 0, epoch 2, cost 100.245657
Pre-training layer 0, epoch 3, cost 99.601676
Pre-training layer 0, epoch 4, cost 99.261929
Pre-training layer 0, epoch 5, cost 99.053529
Pre-training layer 0, epoch 6, cost 98.913443
Pre-training layer 0, epoch 7, cost 98.813258
Pre-training layer 0, epoch 8, cost 98.738321
Pre-training layer 0, epoch 9, cost 98.680326
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.88
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.83539 acc, 0.84722 prec, 0.90148 f1
SVM-RBF 0.85597 acc, 0.84259 prec, 0.91228 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 222.051860
Pre-training layer 0, epoch 1, cost 99.866086
Pre-training layer 0, epoch 2, cost 98.207817
Pre-training layer 0, epoch 3, cost 97.563755
Pre-training layer 0, epoch 4, cost 97.223865
Pre-training layer 0, epoch 5, cost 97.015327
Pre-training layer 0, epoch 6, cost 96.875121
Pre-training layer 0, epoch 7, cost 96.774834
Pre-training layer 0, epoch 8, cost 96.699809
Pre-training layer 0, epoch 9, cost 96.641738
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.90
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.87243 acc, 0.88018 prec, 0.92494 f1
SVM-RBF 0.88477 acc, 0.87558 prec, 0.93137 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 223.382871
Pre-training layer 0, epoch 1, cost 101.407249
Pre-training layer 0, epoch 2, cost 99.752387
Pre-training layer 0, epoch 3, cost 99.110141
Pre-training layer 0, epoch 4, cost 98.771370
Pre-training layer 0, epoch 5, cost 98.563591
Pre-training layer 0, epoch 6, cost 98.423934
Pre-training layer 0, epoch 7, cost 98.324064
Pre-training layer 0, epoch 8, cost 98.249366
Pre-training layer 0, epoch 9, cost 98.191560
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.91
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.79012 acc, 0.80090 prec, 0.87407 f1
SVM-RBF 0.88066 acc, 0.88235 prec, 0.93079 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.76543	0.77333	0.85926
It. 1: 0.84774	0.84753	0.91084
It. 1: 0.83539	0.84722	0.90148
It. 1: 0.87243	0.88018	0.92494
It. 1: 0.79012	0.80090	0.87407

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.81481	0.81333	0.89051
It. 1: 0.87654	0.86996	0.92823
It. 1: 0.85597	0.84259	0.91228
It. 1: 0.88477	0.87558	0.93137
It. 1: 0.88066	0.88235	0.93079

####################################################

SDA (1 layer(s), 0.00 corruption level) performance:
- Accuracy: DT 0.82222 (+/- 0.07798) || SVM-RBF 0.86255 (+/- 0.05169)
- Precision: DT 0.82984 (+/- 0.07579) || SVM-RBF 0.85676 (+/- 0.05114)
- F1: DT 0.89412 (+/- 0.04817) || SVM-RBF 0.91864 (+/- 0.03141)
