Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 137.436984
Pre-training layer 0, epoch 1, cost 102.843868
Pre-training layer 0, epoch 2, cost 101.605615
Pre-training layer 0, epoch 3, cost 100.895034
Pre-training layer 0, epoch 4, cost 100.438333
Pre-training layer 0, epoch 5, cost 100.083583
Pre-training layer 0, epoch 6, cost 99.834124
Pre-training layer 0, epoch 7, cost 99.669664
Pre-training layer 0, epoch 8, cost 99.608404
Pre-training layer 0, epoch 9, cost 99.412144
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10.0, 'gamma': 10.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.98354 acc, 0.99111 prec, 0.99111 f1
SVM-RBF 0.99177 acc, 0.99556 prec, 0.99556 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 135.247527
Pre-training layer 0, epoch 1, cost 100.510743
Pre-training layer 0, epoch 2, cost 99.320774
Pre-training layer 0, epoch 3, cost 98.527640
Pre-training layer 0, epoch 4, cost 98.106686
Pre-training layer 0, epoch 5, cost 97.724859
Pre-training layer 0, epoch 6, cost 97.533099
Pre-training layer 0, epoch 7, cost 97.323916
Pre-training layer 0, epoch 8, cost 97.148445
Pre-training layer 0, epoch 9, cost 97.011096
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97119 acc, 0.97758 prec, 0.98420 f1
SVM-RBF 0.97942 acc, 1.00000 prec, 0.98891 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 136.315790
Pre-training layer 0, epoch 1, cost 101.087669
Pre-training layer 0, epoch 2, cost 99.789402
Pre-training layer 0, epoch 3, cost 99.024114
Pre-training layer 0, epoch 4, cost 98.550075
Pre-training layer 0, epoch 5, cost 98.184576
Pre-training layer 0, epoch 6, cost 98.005964
Pre-training layer 0, epoch 7, cost 97.816538
Pre-training layer 0, epoch 8, cost 97.651365
Pre-training layer 0, epoch 9, cost 97.497672
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.98765 acc, 0.99074 prec, 0.99304 f1
SVM-RBF 0.97942 acc, 0.99074 prec, 0.98845 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 133.831215
Pre-training layer 0, epoch 1, cost 99.062631
Pre-training layer 0, epoch 2, cost 97.780394
Pre-training layer 0, epoch 3, cost 97.062423
Pre-training layer 0, epoch 4, cost 96.580694
Pre-training layer 0, epoch 5, cost 96.294889
Pre-training layer 0, epoch 6, cost 96.042080
Pre-training layer 0, epoch 7, cost 95.872600
Pre-training layer 0, epoch 8, cost 95.715178
Pre-training layer 0, epoch 9, cost 95.603261
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.99177 acc, 0.99539 prec, 0.99539 f1
SVM-RBF 0.98354 acc, 0.99078 prec, 0.99078 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 134.912543
Pre-training layer 0, epoch 1, cost 100.731890
Pre-training layer 0, epoch 2, cost 99.495640
Pre-training layer 0, epoch 3, cost 98.723452
Pre-training layer 0, epoch 4, cost 98.212344
Pre-training layer 0, epoch 5, cost 97.917712
Pre-training layer 0, epoch 6, cost 97.708700
Pre-training layer 0, epoch 7, cost 97.524912
Pre-training layer 0, epoch 8, cost 97.404856
Pre-training layer 0, epoch 9, cost 97.274759
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 0.001} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97119 acc, 0.98643 prec, 0.98420 f1
SVM-RBF 0.98354 acc, 0.99548 prec, 0.99099 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.98354	0.99111	0.99111
It. 1: 0.97119	0.97758	0.98420
It. 1: 0.98765	0.99074	0.99304
It. 1: 0.99177	0.99539	0.99539
It. 1: 0.97119	0.98643	0.98420

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.99177	0.99556	0.99556
It. 1: 0.97942	1.00000	0.98891
It. 1: 0.97942	0.99074	0.98845
It. 1: 0.98354	0.99078	0.99078
It. 1: 0.98354	0.99548	0.99099

####################################################

SDA (1 layer(s), 0.60 corruption level) performance:
- Accuracy: DT 0.98107 (+/- 0.01695) || SVM-RBF 0.98354 (+/- 0.00902)
- Precision: DT 0.98825 (+/- 0.01209) || SVM-RBF 0.99451 (+/- 0.00694)
- F1: DT 0.98959 (+/- 0.00921) || SVM-RBF 0.99094 (+/- 0.00503) 
