Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 147.676862
Pre-training layer 0, epoch 1, cost 106.956291
Pre-training layer 0, epoch 2, cost 103.965414
Pre-training layer 0, epoch 3, cost 102.820218
Pre-training layer 0, epoch 4, cost 101.936813
Pre-training layer 0, epoch 5, cost 101.421838
Pre-training layer 0, epoch 6, cost 100.981306
Pre-training layer 0, epoch 7, cost 100.619006
Pre-training layer 0, epoch 8, cost 100.383280
Pre-training layer 0, epoch 9, cost 100.200481
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.98354 acc, 0.98667 prec, 0.99107 f1
SVM-RBF 0.98765 acc, 0.99556 prec, 0.99335 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 142.234162
Pre-training layer 0, epoch 1, cost 104.399465
Pre-training layer 0, epoch 2, cost 101.626031
Pre-training layer 0, epoch 3, cost 100.429602
Pre-training layer 0, epoch 4, cost 99.615914
Pre-training layer 0, epoch 5, cost 99.044549
Pre-training layer 0, epoch 6, cost 98.711113
Pre-training layer 0, epoch 7, cost 98.400048
Pre-training layer 0, epoch 8, cost 98.165791
Pre-training layer 0, epoch 9, cost 98.011589
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.96708 acc, 0.97309 prec, 0.98190 f1
SVM-RBF 0.97531 acc, 1.00000 prec, 0.98673 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 149.313958
Pre-training layer 0, epoch 1, cost 106.856908
	Pre-training layer 0, epoch 2, cost 103.479041
Pre-training layer 0, epoch 3, cost 101.908141
Pre-training layer 0, epoch 4, cost 101.010748
Pre-training layer 0, epoch 5, cost 100.397663
Pre-training layer 0, epoch 6, cost 99.920674
Pre-training layer 0, epoch 7, cost 99.469896
Pre-training layer 0, epoch 8, cost 99.205278
Pre-training layer 0, epoch 9, cost 99.023846
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97119 acc, 0.98148 prec, 0.98376 f1
SVM-RBF 0.98354 acc, 0.99537 prec, 0.99078 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 142.404173
Pre-training layer 0, epoch 1, cost 102.997217
Pre-training layer 0, epoch 2, cost 99.860437
Pre-training layer 0, epoch 3, cost 98.718667
Pre-training layer 0, epoch 4, cost 98.041047
Pre-training layer 0, epoch 5, cost 97.472621
Pre-training layer 0, epoch 6, cost 97.138765
Pre-training layer 0, epoch 7, cost 96.916172
Pre-training layer 0, epoch 8, cost 96.698620
Pre-training layer 0, epoch 9, cost 96.587529
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.96708 acc, 0.97235 prec, 0.98140 f1
SVM-RBF 0.98354 acc, 0.99539 prec, 0.99083 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 143.144530
Pre-training layer 0, epoch 1, cost 105.128362
Pre-training layer 0, epoch 2, cost 102.191730
Pre-training layer 0, epoch 3, cost 100.764983
Pre-training layer 0, epoch 4, cost 99.944387
Pre-training layer 0, epoch 5, cost 99.434814
Pre-training layer 0, epoch 6, cost 99.074567
Pre-training layer 0, epoch 7, cost 98.651335
Pre-training layer 0, epoch 8, cost 98.356229
Pre-training layer 0, epoch 9, cost 98.211398
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97119 acc, 0.98643 prec, 0.98420 f1
SVM-RBF 0.96708 acc, 0.99095 prec, 0.98206 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.98354	0.98667	0.99107
It. 1: 0.96708	0.97309	0.98190
It. 1: 0.97119	0.98148	0.98376
It. 1: 0.96708	0.97235	0.98140
It. 1: 0.97119	0.98643	0.98420

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.98765	0.99556	0.99335
It. 1: 0.97531	1.00000	0.98673
It. 1: 0.98354	0.99537	0.99078
It. 1: 0.98354	0.99539	0.99083
It. 1: 0.96708	0.99095	0.98206

####################################################

SDA (1 layer(s), 0.30 corruption level) performance:
- Accuracy: DT 0.97202 (+/- 0.01210) || SVM-RBF 0.97942 (+/- 0.01472)
- Precision: DT 0.98000 (+/- 0.01246) || SVM-RBF 0.99545 (+/- 0.00573)
- F1: DT 0.98446 (+/- 0.00694) || SVM-RBF 0.98875 (+/- 0.00792) 
