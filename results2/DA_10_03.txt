Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 417.694229
Pre-training layer 0, epoch 1, cost 120.030072
Pre-training layer 0, epoch 2, cost 110.954791
Pre-training layer 0, epoch 3, cost 107.405114
Pre-training layer 0, epoch 4, cost 105.518879
Pre-training layer 0, epoch 5, cost 104.353648
Pre-training layer 0, epoch 6, cost 103.564673
Pre-training layer 0, epoch 7, cost 102.996499
Pre-training layer 0, epoch 8, cost 102.568791
Pre-training layer 0, epoch 9, cost 102.235560
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.74
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.67078 acc, 0.67556 prec, 0.79167 f1
SVM-RBF 0.53498 acc, 0.50222 prec, 0.66667 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 415.564209
Pre-training layer 0, epoch 1, cost 117.890075
Pre-training layer 0, epoch 2, cost 108.770298
Pre-training layer 0, epoch 3, cost 105.202339
Pre-training layer 0, epoch 4, cost 103.306302
Pre-training layer 0, epoch 5, cost 102.134792
Pre-training layer 0, epoch 6, cost 101.341703
Pre-training layer 0, epoch 7, cost 100.770584
Pre-training layer 0, epoch 8, cost 100.340612
Pre-training layer 0, epoch 9, cost 100.005678
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.84
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.69136 acc, 0.69058 prec, 0.80418 f1
SVM-RBF 0.64198 acc, 0.61435 prec, 0.75900 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 412.245670
Pre-training layer 0, epoch 1, cost 118.088628
Pre-training layer 0, epoch 2, cost 109.099225
Pre-training layer 0, epoch 3, cost 105.582997
Pre-training layer 0, epoch 4, cost 103.714588
Pre-training layer 0, epoch 5, cost 102.560150
Pre-training layer 0, epoch 6, cost 101.778660
Pre-training layer 0, epoch 7, cost 101.216030
Pre-training layer 0, epoch 8, cost 100.792406
Pre-training layer 0, epoch 9, cost 100.462479
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.81
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.74074 acc, 0.76389 prec, 0.83969 f1
SVM-RBF 0.69136 acc, 0.65741 prec, 0.79109 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 410.429516
Pre-training layer 0, epoch 1, cost 116.052615
Pre-training layer 0, epoch 2, cost 107.072778
Pre-training layer 0, epoch 3, cost 103.559335
Pre-training layer 0, epoch 4, cost 101.692065
Pre-training layer 0, epoch 5, cost 100.538231
Pre-training layer 0, epoch 6, cost 99.757150
Pre-training layer 0, epoch 7, cost 99.194455
Pre-training layer 0, epoch 8, cost 98.771065
Pre-training layer 0, epoch 9, cost 98.441132
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000.0, 'gamma': 1000.0} with a score of 0.81
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.69547 acc, 0.69124 prec, 0.80214 f1
SVM-RBF 0.67901 acc, 0.64977 prec, 0.78333 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 410.367162
Pre-training layer 0, epoch 1, cost 117.537519
Pre-training layer 0, epoch 2, cost 108.577756
Pre-training layer 0, epoch 3, cost 105.073249
Pre-training layer 0, epoch 4, cost 103.211207
Pre-training layer 0, epoch 5, cost 102.060748
Pre-training layer 0, epoch 6, cost 101.281954
Pre-training layer 0, epoch 7, cost 100.721282
Pre-training layer 0, epoch 8, cost 100.299021
Pre-training layer 0, epoch 9, cost 99.970275
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000000.0, 'gamma': 1000.0} with a score of 0.72
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.57613 acc, 0.57466 prec, 0.71148 f1
SVM-RBF 0.54733 acc, 0.53394 prec, 0.68208 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.67078	0.67556	0.79167
It. 1: 0.69136	0.69058	0.80418
It. 1: 0.74074	0.76389	0.83969
It. 1: 0.69547	0.69124	0.80214
It. 1: 0.57613	0.57466	0.71148

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.53498	0.50222	0.66667
It. 1: 0.64198	0.61435	0.75900
It. 1: 0.69136	0.65741	0.79109
It. 1: 0.67901	0.64977	0.78333
It. 1: 0.54733	0.53394	0.68208

####################################################

SDA (1 layer(s), 0.30 corruption level) performance:
- Accuracy: DT 0.67490 (+/- 0.10882) || SVM-RBF 0.61893 (+/- 0.13134)
- Precision: DT 0.67919 (+/- 0.12129) || SVM-RBF 0.59154 (+/- 0.12504)
- F1: DT 0.78983 (+/- 0.08478) || SVM-RBF 0.73643 (+/- 0.10399) 

