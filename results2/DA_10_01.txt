Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 219.278562
Pre-training layer 0, epoch 1, cost 103.419086
Pre-training layer 0, epoch 2, cost 101.879200
Pre-training layer 0, epoch 3, cost 101.281310
Pre-training layer 0, epoch 4, cost 100.966438
Pre-training layer 0, epoch 5, cost 100.777289
Pre-training layer 0, epoch 6, cost 100.643554
Pre-training layer 0, epoch 7, cost 100.556472
Pre-training layer 0, epoch 8, cost 100.482297
Pre-training layer 0, epoch 9, cost 100.428369
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.59
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.65021 acc, 0.63556 prec, 0.77089 f1
SVM-RBF 0.28395 acc, 0.23556 prec, 0.37857 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 239.412057
Pre-training layer 0, epoch 1, cost 101.192397
Pre-training layer 0, epoch 2, cost 99.643415
Pre-training layer 0, epoch 3, cost 99.042358
Pre-training layer 0, epoch 4, cost 98.725407
Pre-training layer 0, epoch 5, cost 98.531069
Pre-training layer 0, epoch 6, cost 98.400483
Pre-training layer 0, epoch 7, cost 98.307123
Pre-training layer 0, epoch 8, cost 98.237310
Pre-training layer 0, epoch 9, cost 98.183296
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 0.01, 'gamma': 1.0000000000000001e-09} with a score of 0.50
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.
  'recall', 'true', average, warn_for)
DT 0.08230 acc, 0.00000 prec, 0.00000 f1
SVM-RBF 0.08230 acc, 0.00000 prec, 0.00000 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 239.345022
Pre-training layer 0, epoch 1, cost 101.643407
Pre-training layer 0, epoch 2, cost 100.117857
Pre-training layer 0, epoch 3, cost 99.526101
Pre-training layer 0, epoch 4, cost 99.214148
Pre-training layer 0, epoch 5, cost 99.022924
Pre-training layer 0, epoch 6, cost 98.894461
Pre-training layer 0, epoch 7, cost 98.802637
Pre-training layer 0, epoch 8, cost 98.733988
Pre-training layer 0, epoch 9, cost 98.680883
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.50
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.
  'recall', 'true', average, warn_for)
DT 0.13580 acc, 0.02778 prec, 0.05405 f1
SVM-RBF 0.11111 acc, 0.00000 prec, 0.00000 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 237.885337
Pre-training layer 0, epoch 1, cost 99.601611
Pre-training layer 0, epoch 2, cost 98.076244
Pre-training layer 0, epoch 3, cost 97.483889
Pre-training layer 0, epoch 4, cost 97.171079
Pre-training layer 0, epoch 5, cost 96.980903
Pre-training layer 0, epoch 6, cost 96.851775
Pre-training layer 0, epoch 7, cost 96.760061
Pre-training layer 0, epoch 8, cost 96.692400
Pre-training layer 0, epoch 9, cost 96.639093
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 10.0} with a score of 0.54
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.47325 acc, 0.47926 prec, 0.61905 f1
SVM-RBF 0.76132 acc, 0.80645 prec, 0.85784 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 218.821472
Pre-training layer 0, epoch 1, cost 101.146073
Pre-training layer 0, epoch 2, cost 99.625853
Pre-training layer 0, epoch 3, cost 99.033754
Pre-training layer 0, epoch 4, cost 98.722285
Pre-training layer 0, epoch 5, cost 98.533187
Pre-training layer 0, epoch 6, cost 98.409017
Pre-training layer 0, epoch 7, cost 98.312890
Pre-training layer 0, epoch 8, cost 98.243552
Pre-training layer 0, epoch 9, cost 98.193122
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.77
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.78189 acc, 0.78733 prec, 0.86783 f1
SVM-RBF 0.62963 acc, 0.60633 prec, 0.74860 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.65021	0.63556	0.77089
It. 1: 0.08230	0.00000	0.00000
It. 1: 0.13580	0.02778	0.05405
It. 1: 0.47325	0.47926	0.61905
It. 1: 0.78189	0.78733	0.86783

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.28395	0.23556	0.37857
It. 1: 0.08230	0.00000	0.00000
It. 1: 0.11111	0.00000	0.00000
It. 1: 0.76132	0.80645	0.85784
It. 1: 0.62963	0.60633	0.74860

####################################################

SDA (1 layer(s), 0.10 corruption level) performance:
- Accuracy: DT 0.42469 (+/- 0.55244) || SVM-RBF 0.37366 (+/- 0.54967)
- Precision: DT 0.38599 (+/- 0.63835) || SVM-RBF 0.32967 (+/- 0.65121)
- F1: DT 0.46236 (+/- 0.72918) || SVM-RBF 0.39700 (+/- 0.72197) 
