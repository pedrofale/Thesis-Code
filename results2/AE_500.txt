Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 144.659061
Pre-training layer 0, epoch 1, cost 109.491623
Pre-training layer 0, epoch 2, cost 106.613002
Pre-training layer 0, epoch 3, cost 104.869192
Pre-training layer 0, epoch 4, cost 103.728492
Pre-training layer 0, epoch 5, cost 102.819400
Pre-training layer 0, epoch 6, cost 102.206997
Pre-training layer 0, epoch 7, cost 101.683095
Pre-training layer 0, epoch 8, cost 101.266602
Pre-training layer 0, epoch 9, cost 100.955479
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.98765 acc, 0.99111 prec, 0.99332 f1
SVM-RBF 0.98765 acc, 1.00000 prec, 0.99338 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 142.207078
Pre-training layer 0, epoch 1, cost 106.993948
Pre-training layer 0, epoch 2, cost 104.207116
Pre-training layer 0, epoch 3, cost 102.650324
Pre-training layer 0, epoch 4, cost 101.444006
Pre-training layer 0, epoch 5, cost 100.590210
Pre-training layer 0, epoch 6, cost 99.990138
Pre-training layer 0, epoch 7, cost 99.454861
Pre-training layer 0, epoch 8, cost 99.030383
Pre-training layer 0, epoch 9, cost 98.684857
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.98765 acc, 1.00000 prec, 0.99332 f1
SVM-RBF 0.97531 acc, 0.99552 prec, 0.98667 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 142.525476
Pre-training layer 0, epoch 1, cost 107.597264
Pre-training layer 0, epoch 2, cost 104.791301
Pre-training layer 0, epoch 3, cost 103.106973
Pre-training layer 0, epoch 4, cost 101.914621
Pre-training layer 0, epoch 5, cost 101.021333
Pre-training layer 0, epoch 6, cost 100.435334
Pre-training layer 0, epoch 7, cost 99.842855
Pre-training layer 0, epoch 8, cost 99.459846
Pre-training layer 0, epoch 9, cost 99.130271
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97531 acc, 0.98148 prec, 0.98605 f1
SVM-RBF 0.98354 acc, 0.99537 prec, 0.99078 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 140.474052
Pre-training layer 0, epoch 1, cost 105.722711
Pre-training layer 0, epoch 2, cost 103.007208
Pre-training layer 0, epoch 3, cost 101.353951
Pre-training layer 0, epoch 4, cost 100.028850
Pre-training layer 0, epoch 5, cost 99.143735
Pre-training layer 0, epoch 6, cost 98.551516
Pre-training layer 0, epoch 7, cost 98.028125
Pre-training layer 0, epoch 8, cost 97.650601
Pre-training layer 0, epoch 9, cost 97.292502
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97119 acc, 0.97696 prec, 0.98376 f1
SVM-RBF 0.97942 acc, 0.99539 prec, 0.98856 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 141.949130
Pre-training layer 0, epoch 1, cost 107.170692
Pre-training layer 0, epoch 2, cost 104.523958
Pre-training layer 0, epoch 3, cost 102.768683
Pre-training layer 0, epoch 4, cost 101.518262
Pre-training layer 0, epoch 5, cost 100.665312
Pre-training layer 0, epoch 6, cost 99.984379
Pre-training layer 0, epoch 7, cost 99.496862
Pre-training layer 0, epoch 8, cost 99.100700
Pre-training layer 0, epoch 9, cost 98.757305
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 0.001} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.96708 acc, 0.98643 prec, 0.98198 f1
SVM-RBF 0.97942 acc, 0.99095 prec, 0.98871 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.98765	0.99111	0.99332
It. 1: 0.98765	1.00000	0.99332
It. 1: 0.97531	0.98148	0.98605
It. 1: 0.97119	0.97696	0.98376
It. 1: 0.96708	0.98643	0.98198

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.98765	1.00000	0.99338
It. 1: 0.97531	0.99552	0.98667
It. 1: 0.98354	0.99537	0.99078
It. 1: 0.97942	0.99539	0.98856
It. 1: 0.97942	0.99095	0.98871

####################################################

SDA (1 layer(s), 0.00 corruption level) performance:
- Accuracy: DT 0.97778 (+/- 0.01695) || SVM-RBF 0.98107 (+/- 0.00839)
- Precision: DT 0.98720 (+/- 0.01593) || SVM-RBF 0.99545 (+/- 0.00572)
- F1: DT 0.98768 (+/- 0.00955) || SVM-RBF 0.98962 (+/- 0.00457) 

