Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 217.181478
Pre-training layer 0, epoch 1, cost 103.684366
Pre-training layer 0, epoch 2, cost 102.010101
Pre-training layer 0, epoch 3, cost 101.360110
Pre-training layer 0, epoch 4, cost 101.017212
Pre-training layer 0, epoch 5, cost 100.806894
Pre-training layer 0, epoch 6, cost 100.665528
Pre-training layer 0, epoch 7, cost 100.564437
Pre-training layer 0, epoch 8, cost 100.488828
Pre-training layer 0, epoch 9, cost 100.430319
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.83
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.76543 acc, 0.76889 prec, 0.85856 f1
SVM-RBF 0.74486 acc, 0.72444 prec, 0.84021 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 204.859149
Pre-training layer 0, epoch 1, cost 101.458675
Pre-training layer 0, epoch 2, cost 99.774786
Pre-training layer 0, epoch 3, cost 99.120799
Pre-training layer 0, epoch 4, cost 98.775671
Pre-training layer 0, epoch 5, cost 98.563915
Pre-training layer 0, epoch 6, cost 98.421540
Pre-training layer 0, epoch 7, cost 98.319699
Pre-training layer 0, epoch 8, cost 98.243509
Pre-training layer 0, epoch 9, cost 98.184533
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.88
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.74074 acc, 0.72646 prec, 0.83721 f1
SVM-RBF 0.82305 acc, 0.80717 prec, 0.89330 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 218.071009
Pre-training layer 0, epoch 1, cost 101.904475
Pre-training layer 0, epoch 2, cost 100.245891
Pre-training layer 0, epoch 3, cost 99.601956
Pre-training layer 0, epoch 4, cost 99.262232
Pre-training layer 0, epoch 5, cost 99.053845
Pre-training layer 0, epoch 6, cost 98.913767
Pre-training layer 0, epoch 7, cost 98.813589
Pre-training layer 0, epoch 8, cost 98.738657
Pre-training layer 0, epoch 9, cost 98.680666
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000000.0, 'gamma': 1000.0} with a score of 0.84
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.81481 acc, 0.83333 prec, 0.88889 f1
SVM-RBF 0.76955 acc, 0.75000 prec, 0.85263 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 219.996235
Pre-training layer 0, epoch 1, cost 99.866173
Pre-training layer 0, epoch 2, cost 98.208085
Pre-training layer 0, epoch 3, cost 97.564079
Pre-training layer 0, epoch 4, cost 97.224217
Pre-training layer 0, epoch 5, cost 97.015695
Pre-training layer 0, epoch 6, cost 96.875500
Pre-training layer 0, epoch 7, cost 96.775221
Pre-training layer 0, epoch 8, cost 96.700202
Pre-training layer 0, epoch 9, cost 96.642137
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.85
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.83539 acc, 0.84332 prec, 0.90148 f1
SVM-RBF 0.78601 acc, 0.77880 prec, 0.86667 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 216.345462
Pre-training layer 0, epoch 1, cost 101.407408
Pre-training layer 0, epoch 2, cost 99.752770
Pre-training layer 0, epoch 3, cost 99.110595
Pre-training layer 0, epoch 4, cost 98.771859
Pre-training layer 0, epoch 5, cost 98.564101
Pre-training layer 0, epoch 6, cost 98.424459
Pre-training layer 0, epoch 7, cost 98.324602
Pre-training layer 0, epoch 8, cost 98.249915
Pre-training layer 0, epoch 9, cost 98.192119
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000000.0, 'gamma': 1000.0} with a score of 0.81
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.75309 acc, 0.77828 prec, 0.85149 f1
SVM-RBF 0.73251 acc, 0.71946 prec, 0.83029 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.76543	0.76889	0.85856
It. 1: 0.74074	0.72646	0.83721
It. 1: 0.81481	0.83333	0.88889
It. 1: 0.83539	0.84332	0.90148
It. 1: 0.75309	0.77828	0.85149

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.74486	0.72444	0.84021
It. 1: 0.82305	0.80717	0.89330
It. 1: 0.76955	0.75000	0.85263
It. 1: 0.78601	0.77880	0.86667
It. 1: 0.73251	0.71946	0.83029

####################################################

SDA (1 layer(s), 0.30 corruption level) performance:
- Accuracy: DT 0.78189 (+/- 0.07343) || SVM-RBF 0.77119 (+/- 0.06388)
- Precision: DT 0.79006 (+/- 0.08645) || SVM-RBF 0.75598 (+/- 0.06637)
- F1: DT 0.86752 (+/- 0.04788) || SVM-RBF 0.85662 (+/- 0.04405) 
