Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 211.846810
Pre-training layer 0, epoch 1, cost 103.684295
Pre-training layer 0, epoch 2, cost 102.010135
Pre-training layer 0, epoch 3, cost 101.360171
Pre-training layer 0, epoch 4, cost 101.017286
Pre-training layer 0, epoch 5, cost 100.806974
Pre-training layer 0, epoch 6, cost 100.665613
Pre-training layer 0, epoch 7, cost 100.564524
Pre-training layer 0, epoch 8, cost 100.488917
Pre-training layer 0, epoch 9, cost 100.430409
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.84
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.71193 acc, 0.70222 prec, 0.81865 f1
SVM-RBF 0.67078 acc, 0.64444 prec, 0.78378 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 208.206137
Pre-training layer 0, epoch 1, cost 101.458620
Pre-training layer 0, epoch 2, cost 99.774824
Pre-training layer 0, epoch 3, cost 99.120862
Pre-training layer 0, epoch 4, cost 98.775745
Pre-training layer 0, epoch 5, cost 98.563995
Pre-training layer 0, epoch 6, cost 98.421624
Pre-training layer 0, epoch 7, cost 98.319784
Pre-training layer 0, epoch 8, cost 98.243595
Pre-training layer 0, epoch 9, cost 98.184621
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.82
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.74897 acc, 0.74888 prec, 0.84557 f1
SVM-RBF 0.69959 acc, 0.68161 prec, 0.80637 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 208.941267
Pre-training layer 0, epoch 1, cost 101.904375
Pre-training layer 0, epoch 2, cost 100.245921
Pre-training layer 0, epoch 3, cost 99.602020
Pre-training layer 0, epoch 4, cost 99.262310
Pre-training layer 0, epoch 5, cost 99.053931
Pre-training layer 0, epoch 6, cost 98.913858
Pre-training layer 0, epoch 7, cost 98.813683
Pre-training layer 0, epoch 8, cost 98.738753
Pre-training layer 0, epoch 9, cost 98.680764
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000000.0, 'gamma': 1000.0} with a score of 0.81
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.73663 acc, 0.73611 prec, 0.83246 f1
SVM-RBF 0.71605 acc, 0.68981 prec, 0.81199 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 207.896783
Pre-training layer 0, epoch 1, cost 99.866094
Pre-training layer 0, epoch 2, cost 98.208120
Pre-training layer 0, epoch 3, cost 97.564145
Pre-training layer 0, epoch 4, cost 97.224295
Pre-training layer 0, epoch 5, cost 97.015780
Pre-training layer 0, epoch 6, cost 96.875590
Pre-training layer 0, epoch 7, cost 96.775313
Pre-training layer 0, epoch 8, cost 96.700297
Pre-training layer 0, epoch 9, cost 96.642233
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.77
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.63374 acc, 0.62212 prec, 0.75209 f1
SVM-RBF 0.64609 acc, 0.62212 prec, 0.75843 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 209.768732
Pre-training layer 0, epoch 1, cost 101.407297
Pre-training layer 0, epoch 2, cost 99.752795
Pre-training layer 0, epoch 3, cost 99.110655
Pre-training layer 0, epoch 4, cost 98.771934
Pre-training layer 0, epoch 5, cost 98.564184
Pre-training layer 0, epoch 6, cost 98.424548
Pre-training layer 0, epoch 7, cost 98.324693
Pre-training layer 0, epoch 8, cost 98.250009
Pre-training layer 0, epoch 9, cost 98.192214
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000000.0, 'gamma': 1000.0} with a score of 0.77
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.63374 acc, 0.62896 prec, 0.75749 f1
SVM-RBF 0.65021 acc, 0.61538 prec, 0.76190 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.71193	0.70222	0.81865
It. 1: 0.74897	0.74888	0.84557
It. 1: 0.73663	0.73611	0.83246
It. 1: 0.63374	0.62212	0.75209
It. 1: 0.63374	0.62896	0.75749

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.67078	0.64444	0.78378
It. 1: 0.69959	0.68161	0.80637
It. 1: 0.71605	0.68981	0.81199
It. 1: 0.64609	0.62212	0.75843
It. 1: 0.65021	0.61538	0.76190

####################################################

SDA (1 layer(s), 0.60 corruption level) performance:
- Accuracy: DT 0.69300 (+/- 0.09967) || SVM-RBF 0.67654 (+/- 0.05474)
- Precision: DT 0.68766 (+/- 0.10601) || SVM-RBF 0.65068 (+/- 0.06059)
- F1: DT 0.80125 (+/- 0.07783) || SVM-RBF 0.78449 (+/- 0.04404) 

