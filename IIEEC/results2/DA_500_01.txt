Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 144.564203
Pre-training layer 0, epoch 1, cost 109.720051
Pre-training layer 0, epoch 2, cost 106.782247
Pre-training layer 0, epoch 3, cost 105.024816
Pre-training layer 0, epoch 4, cost 103.797100
Pre-training layer 0, epoch 5, cost 102.982946
Pre-training layer 0, epoch 6, cost 102.315106
Pre-training layer 0, epoch 7, cost 101.883782
Pre-training layer 0, epoch 8, cost 101.450652
Pre-training layer 0, epoch 9, cost 101.111342
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97942 acc, 0.98222 prec, 0.98881 f1
SVM-RBF 0.98765 acc, 1.00000 prec, 0.99338 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 142.149314
Pre-training layer 0, epoch 1, cost 107.275818
Pre-training layer 0, epoch 2, cost 104.444099
Pre-training layer 0, epoch 3, cost 102.744472
Pre-training layer 0, epoch 4, cost 101.636640
Pre-training layer 0, epoch 5, cost 100.805711
Pre-training layer 0, epoch 6, cost 100.054043
Pre-training layer 0, epoch 7, cost 99.560604
Pre-training layer 0, epoch 8, cost 99.161078
Pre-training layer 0, epoch 9, cost 98.837531
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97119 acc, 0.97758 prec, 0.98420 f1
SVM-RBF 0.97531 acc, 0.99552 prec, 0.98667 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 142.268675
Pre-training layer 0, epoch 1, cost 107.758092
Pre-training layer 0, epoch 2, cost 104.937303
Pre-training layer 0, epoch 3, cost 103.288049
Pre-training layer 0, epoch 4, cost 102.109118
Pre-training layer 0, epoch 5, cost 101.157135
Pre-training layer 0, epoch 6, cost 100.533419
Pre-training layer 0, epoch 7, cost 100.030037
Pre-training layer 0, epoch 8, cost 99.656258
Pre-training layer 0, epoch 9, cost 99.312921
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97531 acc, 0.99074 prec, 0.98618 f1
SVM-RBF 0.98354 acc, 0.99537 prec, 0.99078 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 140.353087
Pre-training layer 0, epoch 1, cost 105.829956
Pre-training layer 0, epoch 2, cost 103.117736
Pre-training layer 0, epoch 3, cost 101.470021
Pre-training layer 0, epoch 4, cost 100.173380
Pre-training layer 0, epoch 5, cost 99.331729
Pre-training layer 0, epoch 6, cost 98.650284
Pre-training layer 0, epoch 7, cost 98.198031
Pre-training layer 0, epoch 8, cost 97.793041
Pre-training layer 0, epoch 9, cost 97.461226
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 10.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.98354 acc, 0.98157 prec, 0.99070 f1
SVM-RBF 0.97942 acc, 0.99078 prec, 0.98851 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 141.856908
Pre-training layer 0, epoch 1, cost 107.254690
Pre-training layer 0, epoch 2, cost 104.700864
Pre-training layer 0, epoch 3, cost 102.881702
Pre-training layer 0, epoch 4, cost 101.687308
Pre-training layer 0, epoch 5, cost 100.876339
Pre-training layer 0, epoch 6, cost 100.186276
Pre-training layer 0, epoch 7, cost 99.700984
Pre-training layer 0, epoch 8, cost 99.340419
Pre-training layer 0, epoch 9, cost 98.968813
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 0.001} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.97119 acc, 0.98643 prec, 0.98420 f1
SVM-RBF 0.97942 acc, 0.99095 prec, 0.98871 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.97942	0.98222	0.98881
It. 1: 0.97119	0.97758	0.98420
It. 1: 0.97531	0.99074	0.98618
It. 1: 0.98354	0.98157	0.99070
It. 1: 0.97119	0.98643	0.98420

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.98765	1.00000	0.99338
It. 1: 0.97531	0.99552	0.98667
It. 1: 0.98354	0.99537	0.99078
It. 1: 0.97942	0.99078	0.98851
It. 1: 0.97942	0.99095	0.98871

####################################################

SDA (1 layer(s), 0.10 corruption level) performance:
- Accuracy: DT 0.97613 (+/- 0.00960) || SVM-RBF 0.98107 (+/- 0.00839)
- Precision: DT 0.98371 (+/- 0.00900) || SVM-RBF 0.99452 (+/- 0.00684)
- F1: DT 0.98682 (+/- 0.00515) || SVM-RBF 0.98961 (+/- 0.00458)
