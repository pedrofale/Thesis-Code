Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 490.733923
Pre-training layer 0, epoch 1, cost 121.921378
Pre-training layer 0, epoch 2, cost 111.976873
Pre-training layer 0, epoch 3, cost 108.093694
Pre-training layer 0, epoch 4, cost 106.031610
Pre-training layer 0, epoch 5, cost 104.757913
Pre-training layer 0, epoch 6, cost 103.895526
Pre-training layer 0, epoch 7, cost 103.274687
Pre-training layer 0, epoch 8, cost 102.807184
Pre-training layer 0, epoch 9, cost 102.443146
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 100.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.72428 acc, 0.71556 prec, 0.82776 f1
SVM-RBF 0.99177 acc, 0.99111 prec, 0.99554 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 488.776929
Pre-training layer 0, epoch 1, cost 119.799749
Pre-training layer 0, epoch 2, cost 109.797179
Pre-training layer 0, epoch 3, cost 105.892200
Pre-training layer 0, epoch 4, cost 103.818128
Pre-training layer 0, epoch 5, cost 102.537069
Pre-training layer 0, epoch 6, cost 101.669894
Pre-training layer 0, epoch 7, cost 101.045302
Pre-training layer 0, epoch 8, cost 100.575130
Pre-training layer 0, epoch 9, cost 100.208948
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 100.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.69547 acc, 0.68610 prec, 0.80526 f1
SVM-RBF 0.97531 acc, 0.97758 prec, 0.98643 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 485.844195
Pre-training layer 0, epoch 1, cost 119.942542
Pre-training layer 0, epoch 2, cost 110.099216
Pre-training layer 0, epoch 3, cost 106.255590
Pre-training layer 0, epoch 4, cost 104.214465
Pre-training layer 0, epoch 5, cost 102.953496
Pre-training layer 0, epoch 6, cost 102.099946
Pre-training layer 0, epoch 7, cost 101.485264
Pre-training layer 0, epoch 8, cost 101.022452
Pre-training layer 0, epoch 9, cost 100.661963
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 100.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.76955 acc, 0.79167 prec, 0.85930 f1
SVM-RBF 0.98765 acc, 0.99537 prec, 0.99307 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 482.619803
Pre-training layer 0, epoch 1, cost 117.901692
Pre-training layer 0, epoch 2, cost 108.072507
Pre-training layer 0, epoch 3, cost 104.232722
Pre-training layer 0, epoch 4, cost 102.192793
Pre-training layer 0, epoch 5, cost 100.932562
Pre-training layer 0, epoch 6, cost 100.079197
Pre-training layer 0, epoch 7, cost 99.464788
Pre-training layer 0, epoch 8, cost 99.002039
Pre-training layer 0, epoch 9, cost 98.641644
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000.0, 'gamma': 1000.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.80247 acc, 0.81106 prec, 0.88000 f1
SVM-RBF 0.98765 acc, 0.98618 prec, 0.99304 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 483.137708
Pre-training layer 0, epoch 1, cost 119.389829
Pre-training layer 0, epoch 2, cost 109.579686
Pre-training layer 0, epoch 3, cost 105.748411
Pre-training layer 0, epoch 4, cost 103.713680
Pre-training layer 0, epoch 5, cost 102.456593
Pre-training layer 0, epoch 6, cost 101.605635
Pre-training layer 0, epoch 7, cost 100.992941
Pre-training layer 0, epoch 8, cost 100.531558
Pre-training layer 0, epoch 9, cost 100.172234
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000.0, 'gamma': 100.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.74074 acc, 0.75566 prec, 0.84131 f1
SVM-RBF 0.96296 acc, 0.97738 prec, 0.97959 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.72428	0.71556	0.82776
It. 1: 0.69547	0.68610	0.80526
It. 1: 0.76955	0.79167	0.85930
It. 1: 0.80247	0.81106	0.88000
It. 1: 0.74074	0.75566	0.84131

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.99177	0.99111	0.99554
It. 1: 0.97531	0.97758	0.98643
It. 1: 0.98765	0.99537	0.99307
It. 1: 0.98765	0.98618	0.99304
It. 1: 0.96296	0.97738	0.97959

####################################################

SDA (1 layer(s), 0.10 corruption level) performance:
- Accuracy: DT 0.74650 (+/- 0.07373) || SVM-RBF 0.98107 (+/- 0.02121)
- Precision: DT 0.75201 (+/- 0.09272) || SVM-RBF 0.98552 (+/- 0.01437)
- F1: DT 0.84273 (+/- 0.05133) || SVM-RBF 0.98953 (+/- 0.01164) 

