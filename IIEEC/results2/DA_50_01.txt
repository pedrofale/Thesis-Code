Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 224.015048
Pre-training layer 0, epoch 1, cost 103.684363
Pre-training layer 0, epoch 2, cost 102.010019
Pre-training layer 0, epoch 3, cost 101.360003
Pre-training layer 0, epoch 4, cost 101.017096
Pre-training layer 0, epoch 5, cost 100.806771
Pre-training layer 0, epoch 6, cost 100.665402
Pre-training layer 0, epoch 7, cost 100.564308
Pre-training layer 0, epoch 8, cost 100.488697
Pre-training layer 0, epoch 9, cost 100.430186
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 1000.0} with a score of 0.81
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.72840 acc, 0.72889 prec, 0.83249 f1
SVM-RBF 0.65844 acc, 0.63111 prec, 0.77384 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 221.555303
Pre-training layer 0, epoch 1, cost 101.458666
Pre-training layer 0, epoch 2, cost 99.774642
Pre-training layer 0, epoch 3, cost 99.120615
Pre-training layer 0, epoch 4, cost 98.775469
Pre-training layer 0, epoch 5, cost 98.563701
Pre-training layer 0, epoch 6, cost 98.421320
Pre-training layer 0, epoch 7, cost 98.319473
Pre-training layer 0, epoch 8, cost 98.243278
Pre-training layer 0, epoch 9, cost 98.184301
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.87
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.78189 acc, 0.80269 prec, 0.87105 f1
SVM-RBF 0.79835 acc, 0.79372 prec, 0.87841 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 214.360596
Pre-training layer 0, epoch 1, cost 101.904425
Pre-training layer 0, epoch 2, cost 100.245753
Pre-training layer 0, epoch 3, cost 99.601792
Pre-training layer 0, epoch 4, cost 99.262055
Pre-training layer 0, epoch 5, cost 99.053660
Pre-training layer 0, epoch 6, cost 98.913577
Pre-training layer 0, epoch 7, cost 98.813395
Pre-training layer 0, epoch 8, cost 98.738460
Pre-training layer 0, epoch 9, cost 98.680467
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.90
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.85185 acc, 0.87963 prec, 0.91346 f1
SVM-RBF 0.87654 acc, 0.87037 prec, 0.92611 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 215.734242
Pre-training layer 0, epoch 1, cost 99.866151
Pre-training layer 0, epoch 2, cost 98.207982
Pre-training layer 0, epoch 3, cost 97.563953
Pre-training layer 0, epoch 4, cost 97.224078
Pre-training layer 0, epoch 5, cost 97.015550
Pre-training layer 0, epoch 6, cost 96.875351
Pre-training layer 0, epoch 7, cost 96.775069
Pre-training layer 0, epoch 8, cost 96.700048
Pre-training layer 0, epoch 9, cost 96.641981
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.89
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.88066 acc, 0.89401 prec, 0.93046 f1
SVM-RBF 0.84774 acc, 0.84332 prec, 0.90819 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 219.740902
Pre-training layer 0, epoch 1, cost 101.407393
Pre-training layer 0, epoch 2, cost 99.752650
Pre-training layer 0, epoch 3, cost 99.110443
Pre-training layer 0, epoch 4, cost 98.771693
Pre-training layer 0, epoch 5, cost 98.563928
Pre-training layer 0, epoch 6, cost 98.424281
Pre-training layer 0, epoch 7, cost 98.324419
Pre-training layer 0, epoch 8, cost 98.249730
Pre-training layer 0, epoch 9, cost 98.191930
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000000.0, 'gamma': 1000.0} with a score of 0.92
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.80247 acc, 0.80090 prec, 0.88060 f1
SVM-RBF 0.89300 acc, 0.89140 prec, 0.93810 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.72840	0.72889	0.83249
It. 1: 0.78189	0.80269	0.87105
It. 1: 0.85185	0.87963	0.91346
It. 1: 0.88066	0.89401	0.93046
It. 1: 0.80247	0.80090	0.88060

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.65844	0.63111	0.77384
It. 1: 0.79835	0.79372	0.87841
It. 1: 0.87654	0.87037	0.92611
It. 1: 0.84774	0.84332	0.90819
It. 1: 0.89300	0.89140	0.93810

####################################################

SDA (1 layer(s), 0.10 corruption level) performance:
- Accuracy: DT 0.80905 (+/- 0.10676) || SVM-RBF 0.81481 (+/- 0.16908)
- Precision: DT 0.82122 (+/- 0.11997) || SVM-RBF 0.80598 (+/- 0.18669)
- F1: DT 0.88561 (+/- 0.06839) || SVM-RBF 0.88493 (+/- 0.11814) 

