Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 220.727256
Pre-training layer 0, epoch 1, cost 103.418475
Pre-training layer 0, epoch 2, cost 101.877994
Pre-training layer 0, epoch 3, cost 101.282212
Pre-training layer 0, epoch 4, cost 100.965287
Pre-training layer 0, epoch 5, cost 100.783990
Pre-training layer 0, epoch 6, cost 100.644135
Pre-training layer 0, epoch 7, cost 100.551455
Pre-training layer 0, epoch 8, cost 100.481957
Pre-training layer 0, epoch 9, cost 100.427512
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000.0, 'gamma': 1000.0} with a score of 0.69
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.63374 acc, 0.62667 prec, 0.76011 f1
SVM-RBF 0.49794 acc, 0.46222 prec, 0.63030 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 247.573750
Pre-training layer 0, epoch 1, cost 101.190643
Pre-training layer 0, epoch 2, cost 99.640734
Pre-training layer 0, epoch 3, cost 99.039231
Pre-training layer 0, epoch 4, cost 98.723051
Pre-training layer 0, epoch 5, cost 98.528433
Pre-training layer 0, epoch 6, cost 98.397715
Pre-training layer 0, epoch 7, cost 98.304283
Pre-training layer 0, epoch 8, cost 98.234425
Pre-training layer 0, epoch 9, cost 98.180371
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.68
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.75720 acc, 0.76233 prec, 0.85213 f1
SVM-RBF 0.89712 acc, 0.94619 prec, 0.94407 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 210.787504
Pre-training layer 0, epoch 1, cost 101.642370
Pre-training layer 0, epoch 2, cost 100.116019
Pre-training layer 0, epoch 3, cost 99.524475
Pre-training layer 0, epoch 4, cost 99.210048
Pre-training layer 0, epoch 5, cost 99.021272
Pre-training layer 0, epoch 6, cost 98.894494
Pre-training layer 0, epoch 7, cost 98.799191
Pre-training layer 0, epoch 8, cost 98.728024
Pre-training layer 0, epoch 9, cost 98.682288
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000000.0, 'gamma': 1000.0} with a score of 0.76
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.65844 acc, 0.65741 prec, 0.77384 f1
SVM-RBF 0.53909 acc, 0.49074 prec, 0.65432 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 236.177014
Pre-training layer 0, epoch 1, cost 99.602068
Pre-training layer 0, epoch 2, cost 98.077064
Pre-training layer 0, epoch 3, cost 97.485176
Pre-training layer 0, epoch 4, cost 97.172910
Pre-training layer 0, epoch 5, cost 96.980652
Pre-training layer 0, epoch 6, cost 96.851612
Pre-training layer 0, epoch 7, cost 96.759998
Pre-training layer 0, epoch 8, cost 96.691062
Pre-training layer 0, epoch 9, cost 96.637731
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1.0, 'gamma': 1000.0} with a score of 0.53
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.54321 acc, 0.52995 prec, 0.67449 f1
SVM-RBF 0.85185 acc, 0.92627 prec, 0.91781 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 218.510636
Pre-training layer 0, epoch 1, cost 101.147062
Pre-training layer 0, epoch 2, cost 99.625493
Pre-training layer 0, epoch 3, cost 99.035440
Pre-training layer 0, epoch 4, cost 98.724428
Pre-training layer 0, epoch 5, cost 98.533799
Pre-training layer 0, epoch 6, cost 98.405746
Pre-training layer 0, epoch 7, cost 98.314222
Pre-training layer 0, epoch 8, cost 98.245802
Pre-training layer 0, epoch 9, cost 98.192878
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000000000.0, 'gamma': 0.001} with a score of 0.51
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.
  'recall', 'true', average, warn_for)
DT 0.15638 acc, 0.07240 prec, 0.13502 f1
SVM-RBF 0.09053 acc, 0.00000 prec, 0.00000 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.63374	0.62667	0.76011
It. 1: 0.75720	0.76233	0.85213
It. 1: 0.65844	0.65741	0.77384
It. 1: 0.54321	0.52995	0.67449
It. 1: 0.15638	0.07240	0.13502

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.49794	0.46222	0.63030
It. 1: 0.89712	0.94619	0.94407
It. 1: 0.53909	0.49074	0.65432
It. 1: 0.85185	0.92627	0.91781
It. 1: 0.09053	0.00000	0.00000

####################################################

SDA (1 layer(s), 0.00 corruption level) performance:
- Accuracy: DT 0.54979 (+/- 0.41635) || SVM-RBF 0.57531 (+/- 0.58127)
- Precision: DT 0.52975 (+/- 0.48079) || SVM-RBF 0.56508 (+/- 0.69921)
- F1: DT 0.63912 (+/- 0.51655) || SVM-RBF 0.62930 (+/- 0.68057) 

