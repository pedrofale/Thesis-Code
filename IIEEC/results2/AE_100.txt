Removing cached data...
done
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
Oversampling the training set...
--- iteration no. 1 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 493.626081
Pre-training layer 0, epoch 1, cost 121.885277
Pre-training layer 0, epoch 2, cost 111.953151
Pre-training layer 0, epoch 3, cost 108.076637
Pre-training layer 0, epoch 4, cost 106.017966
Pre-training layer 0, epoch 5, cost 104.746258
Pre-training layer 0, epoch 6, cost 103.885363
Pre-training layer 0, epoch 7, cost 103.265427
Pre-training layer 0, epoch 8, cost 102.798638
Pre-training layer 0, epoch 9, cost 102.435080
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000.0, 'gamma': 100.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.76543 acc, 0.75556 prec, 0.85642 f1
SVM-RBF 0.97119 acc, 0.97333 prec, 0.98427 f1
--- iteration no. 2 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 491.460611
Pre-training layer 0, epoch 1, cost 119.766678
Pre-training layer 0, epoch 2, cost 109.775263
Pre-training layer 0, epoch 3, cost 105.875446
Pre-training layer 0, epoch 4, cost 103.804521
Pre-training layer 0, epoch 5, cost 102.525283
Pre-training layer 0, epoch 6, cost 101.659300
Pre-training layer 0, epoch 7, cost 101.035696
Pre-training layer 0, epoch 8, cost 100.566140
Pre-training layer 0, epoch 9, cost 100.200418
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 100.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.67078 acc, 0.67265 prec, 0.78947 f1
SVM-RBF 0.99177 acc, 0.99552 prec, 0.99552 f1
--- iteration no. 3 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 488.218220
Pre-training layer 0, epoch 1, cost 119.908234
Pre-training layer 0, epoch 2, cost 110.076044
Pre-training layer 0, epoch 3, cost 106.238597
Pre-training layer 0, epoch 4, cost 104.200636
Pre-training layer 0, epoch 5, cost 102.941684
Pre-training layer 0, epoch 6, cost 102.089404
Pre-training layer 0, epoch 7, cost 101.475662
Pre-training layer 0, epoch 8, cost 101.013523
Pre-training layer 0, epoch 9, cost 100.653583
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 100000.0, 'gamma': 1000.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.79835 acc, 0.81481 prec, 0.87781 f1
SVM-RBF 0.98354 acc, 0.99074 prec, 0.99074 f1
--- iteration no. 4 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 485.542812
Pre-training layer 0, epoch 1, cost 117.854909
Pre-training layer 0, epoch 2, cost 108.043956
Pre-training layer 0, epoch 3, cost 104.212355
Pre-training layer 0, epoch 4, cost 102.176842
Pre-training layer 0, epoch 5, cost 100.919119
Pre-training layer 0, epoch 6, cost 100.067531
Pre-training layer 0, epoch 7, cost 99.454203
Pre-training layer 0, epoch 8, cost 98.992324
Pre-training layer 0, epoch 9, cost 98.632543
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 1000000.0, 'gamma': 100.0} with a score of 1.00
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.76543 acc, 0.77419 prec, 0.85496 f1
SVM-RBF 0.99177 acc, 0.99078 prec, 0.99537 f1
--- iteration no. 5 ---
building SDA...
getting the pretraining functions...
pre-training the SDA...
Pre-training layer 0, epoch 0, cost 485.793895
Pre-training layer 0, epoch 1, cost 119.349015
Pre-training layer 0, epoch 2, cost 109.553621
Pre-training layer 0, epoch 3, cost 105.729631
Pre-training layer 0, epoch 4, cost 103.698666
Pre-training layer 0, epoch 5, cost 102.443996
Pre-training layer 0, epoch 6, cost 101.594601
Pre-training layer 0, epoch 7, cost 100.982914
Pre-training layer 0, epoch 8, cost 100.522331
Pre-training layer 0, epoch 9, cost 100.163597
getting the finetuning functions...
training decision tree...
finding parameters for SVM-RBF...
...the best parameters are {'C': 10000.0, 'gamma': 1000.0} with a score of 0.99
training an SVM-RBF with optimal parameters...
evaluating decision tree...
evaluating decision tree...
DT 0.80658 acc, 0.81448 prec, 0.88452 f1
SVM-RBF 0.96708 acc, 0.98190 prec, 0.98190 f1
### All results: ###
	Acc	Prec	F1
- DT:
It. 1: 0.76543	0.75556	0.85642
It. 1: 0.67078	0.67265	0.78947
It. 1: 0.79835	0.81481	0.87781
It. 1: 0.76543	0.77419	0.85496
It. 1: 0.80658	0.81448	0.88452

	Acc	Prec	F1
- SVM-RBF:
It. 1: 0.97119	0.97333	0.98427
It. 1: 0.99177	0.99552	0.99552
It. 1: 0.98354	0.99074	0.99074
It. 1: 0.99177	0.99078	0.99537
It. 1: 0.96708	0.98190	0.98190

####################################################

SDA (1 layer(s), 0.00 corruption level) performance:
- Accuracy: DT 0.76132 (+/- 0.09655) || SVM-RBF 0.98107 (+/- 0.02056)
- Precision: DT 0.76634 (+/- 0.10440) || SVM-RBF 0.98645 (+/- 0.01580)
- F1: DT 0.85264 (+/- 0.06729) || SVM-RBF 0.98956 (+/- 0.01122) 

